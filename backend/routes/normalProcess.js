// routes/normalProcess.js
const express = require("express");
const router = express.Router();
const path = require("path");
const fs = require("fs");
const { spawn } = require("child_process");
const { upload } = require("../middleware/upload");

const rootDir = path.join(__dirname, "..");

const loadJsonSafe = (filePath) => {
  try {
    const fullPath = path.join(rootDir, filePath);
    const raw = fs.readFileSync(fullPath, "utf8");
    return JSON.parse(raw);
  } catch (err) {
    return [];
  }
};

const generateGraphData = (pList, mList, oList) => {
  const nodes = [];
  const edges = [];
  let lastNodeId = "dataset-node";
  let xPos = 50; 
  
  nodes.push({
    id: "dataset-node",
    type: "datasetNode",
    position: { x: xPos, y: 100 },
    data: { label: "Dataset" },
  });
  xPos += 250;

  const allPreproc = loadJsonSafe("preprocessing/Normal_preprocessing/normal_preprocessing_modules.json");
  const allModels = loadJsonSafe("model_selectionAndTraining/model_names.json");
  const allOutputs = loadJsonSafe("output_section/output_options.json");

  pList.forEach((id) => {
    const module = allPreproc.find(m => m.id === id);
    if (!module) return;
    const newNodeId = `p_${id}_${Date.now()}`;
    nodes.push({ id: newNodeId, type: "preprocessingNode", position: { x: xPos, y: 100 }, data: { label: module.label, baseId: id } });
    edges.push({ id: `e-${lastNodeId}-${newNodeId}`, source: lastNodeId, target: newNodeId, animated: true });
    lastNodeId = newNodeId;
    xPos += 250;
  });

  mList.forEach((id) => {
    const module = allModels.find(m => m.id === id);
    if (!module) return;
    const newNodeId = `m_${id}_${Date.now()}`;
    nodes.push({ id: newNodeId, type: "modelNode", position: { x: xPos, y: 100 }, data: { label: module.label, baseId: id } });
    edges.push({ id: `e-${lastNodeId}-${newNodeId}`, source: lastNodeId, target: newNodeId, animated: true });
    lastNodeId = newNodeId;
    xPos += 250;
  });

  oList.forEach((id) => {
    const module = allOutputs.find(m => m.id === id);
    if (!module) return;
    const newNodeId = `o_${id}_${Date.now()}`;
    nodes.push({ id: newNodeId, type: "outputNode", position: { x: xPos, y: 85 }, data: { label: module.label, baseId: id } });
    edges.push({ id: `e-${lastNodeId}-${newNodeId}`, source: lastNodeId, target: newNodeId, animated: true });
    lastNodeId = newNodeId;
    xPos += 250;
  });

  return { nodes, edges };
};

// --- UPDATED HELPER: Captures Error Detail ---
const runPythonScript = (scriptPath, args) => {
  return new Promise((resolve, reject) => {
    const python = spawn("python", [scriptPath, ...args]);
    let output = "";
    let errorOutput = "";

    python.stdout.on("data", (data) => { output += data.toString(); });
    python.stderr.on("data", (data) => { errorOutput += data.toString(); });

    python.on("close", (code) => {
      if (code === 0) {
        resolve(output);
      } else {
        // REJECT WITH THE SPECIFIC ERROR FROM PYTHON
        // We trim it to avoid huge logs, but keep the core error message.
        console.error(`[Py-Err] ${scriptPath} failed:\n${errorOutput}`);
        reject(new Error(errorOutput || `Script exited with code ${code}`));
      }
    });
  });
};

const processBranch = async (branchName, datasetPath, pList, mList, oList) => {
  console.log(`\nðŸŒ¿ Processing Branch: ${branchName}`);
  
  const logDirName = `${branchName}_logging`;
  const logDirPath = path.join(rootDir, "preprocessing", "Normal_preprocessing", logDirName);
  
  // Clean logic handled inside python script now
  
  const outputCsvName = `${branchName}_processed.csv`;
  const preprocessedPath = path.join(rootDir, outputCsvName);

  const allModules = loadJsonSafe("preprocessing/Normal_preprocessing/normal_preprocessing_modules.json");
  const modulesToUse = pList.map(id => allModules.find(m => m.id === id)).filter(Boolean);

  // A. RUN PREPROCESSING
  try {
    await runPythonScript(
      "preprocessing/Normal_preprocessing/normal_preprocessing_handler.py",
      [
        datasetPath,
        JSON.stringify(modulesToUse),
        preprocessedPath,
        logDirPath
      ]
    );
    console.log(`   âœ… Preprocessing Complete.`);
  } catch (err) {
    // --- THROW SPECIFIC ERROR ---
    throw new Error(`Preprocessing Failed: ${err.message.split('\n').pop()}`); 
  }

  // B. RUN MODEL TRAINING
  let trainingResults = [];
  let trainedModelPath = null;

  if (mList.length > 0) {
    try {
      const allModels = loadJsonSafe("model_selectionAndTraining/model_names.json");
      const selectedModels = allModels.filter(m => mList.includes(m.id) || mList.includes(m.baseId));
      
      if (selectedModels.length > 0) {
        const output = await runPythonScript(
          "model_selectionAndTraining/model_handler.py",
          [preprocessedPath, JSON.stringify(selectedModels)]
        );

        const jsonStart = output.indexOf("__JSON_START__");
        const jsonEnd = output.indexOf("__JSON_END__");
        if (jsonStart !== -1 && jsonEnd !== -1) {
            const jsonStr = output.substring(jsonStart + 14, jsonEnd);
            trainingResults = JSON.parse(jsonStr);
            if (trainingResults.length > 0) {
                trainedModelPath = trainingResults[0].path;
            }
        }
        console.log(`   âœ… Model Training Complete.`);
      }
    } catch (err) {
      // --- THROW SPECIFIC ERROR ---
      // This catches the "string to float" error
      throw new Error(`Model Training Failed: ${err.message.split('\n').slice(-2).join(' ')}`);
    }
  }

  // C. RUN OUTPUT GENERATION
  let visualizationData = {};
  if (oList.length > 0 && trainedModelPath) {
    try {
      const output = await runPythonScript(
        "output_section/output_handler.py",
        [preprocessedPath, trainedModelPath, JSON.stringify(oList)]
      );

      const jsonStart = output.indexOf("__JSON_START__");
      const jsonEnd = output.indexOf("__JSON_END__");
      if (jsonStart !== -1 && jsonEnd !== -1) {
          const jsonStr = output.substring(jsonStart + 14, jsonEnd);
          visualizationData = JSON.parse(jsonStr);
      }
      console.log(`   âœ… Output Generation Complete.`);
    } catch (err) {
       throw new Error(`Output Generation Failed: ${err.message}`);
    }
  }

  const graphData = generateGraphData(pList, mList, oList);

  return {
    outputs: visualizationData,
    trainingResults: trainingResults,
    graph: graphData
  };
};

router.post("/preprocess-normal", upload.single("dataset"), async (req, res) => {
  if (!req.file) return res.status(400).json({ error: "Dataset file missing" });

  const isCustom = req.body.isCustom === "true";
  let customIds = req.body.ids ? JSON.parse(req.body.ids) : [];
  let modelIds = req.body.modelIds ? JSON.parse(req.body.modelIds) : [];
  let outputIds = req.body.outputIds ? JSON.parse(req.body.outputIds) : [];

  if (!isCustom) {
     modelIds = ['m1'];
     outputIds = ['o1'];
     const allModules = loadJsonSafe("preprocessing/Normal_preprocessing/normal_preprocessing_modules.json");
     if (customIds.length === 0) customIds = allModules.map(m => m.id);
  }

  try {
    const result = await processBranch("main_branch", req.file.path, customIds, modelIds, outputIds);
    res.json({ message: "Pipeline Completed Successfully", ...result });
  } catch (err) {
    res.status(500).json({ message: "Pipeline Processing Failed", error: err.message });
  }
});

module.exports = { router, processBranch };